{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjfVc2rREh9Qc4KSgxGhxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/letsbegincode/ML_algo/blob/main/gradient_descent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#  Implementation of Gradient Descent\n",
        "\n"
      ],
      "metadata": {
        "id": "jYoJ49UTWoar"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QPkpr4eBWn-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Descent Variants\n",
        "\n",
        "## Introduction\n",
        "Gradient Descent is an optimization algorithm used to minimize functions, commonly used in machine learning to minimize the cost function. Below, we cover various types of gradient descent methods, including Batch Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent."
      ],
      "metadata": {
        "id": "9PKheIMXbU18"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "L0w7YJ6sbWFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1: Generating a Random Dataset and Finding Coefficients Using Gradient Descent\n",
        "\n",
        "## Objective\n",
        "In this task, we will:\n",
        "1. Generate a random dataset using the `make_regression` function from `sklearn`.\n",
        "2. Use gradient descent to find the coefficients (`coef_`) and intercept (`intercept_`) of a linear regression model.\n"
      ],
      "metadata": {
        "id": "im9CH_kyaGqL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k5qGPrkzW50G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FxkQsfFrLfaC"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_regression(n_samples=100,n_features=1,n_informative=1,n_targets=1,noise=20,random_state=13)"
      ],
      "metadata": {
        "id": "7B-hyJt9NUON"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(2,2))\n",
        "plt.scatter(X,y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "_IU50ujnNT91",
        "outputId": "5c735765-7ee4-4eab-84d1-548134c51dbb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 200x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state= 2)"
      ],
      "metadata": {
        "id": "EapGvJRKO6W3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "7UJp90yiO6NM",
        "outputId": "f3626b3e-4bfc-46f3-f626-cfa4d524cbd8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = lr.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ra0xPh9Pb2y",
        "outputId": "8d7320e0-93c9-420d-b5f2-65490d19e53e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6345158782661012"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ayx_7C9iPbt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6UWBzvTBPoEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extension by first by making class for fit and predict using formula -\n",
        "\n",
        "### Mean Squared Error Cost Function\n",
        "$$\n",
        "J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
        "$$\n",
        "\n",
        "### Gradient with respect to \\( m \\)\n",
        "$$\n",
        "\\frac{\\partial J(m, b)}{\\partial m} = -\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Gradient with respect to \\( b \\)\n",
        "$$\n",
        "\\frac{\\partial J(m, b)}{\\partial b} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( m \\)\n",
        "$$\n",
        "m := m - \\alpha \\frac{\\partial J(m, b)}{\\partial m}\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( b \\)\n",
        "$$\n",
        "b := b - \\alpha \\frac{\\partial J(m, b)}{\\partial b}\n",
        "$$\n",
        "\n",
        "Gradient with Respect to\n",
        "\n",
        "\n",
        "### Update Rule for \\( m \\)\n",
        "$$\n",
        "m := m - \\alpha \\left( -\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( b \\)\n",
        "$$\n",
        "b := b - \\alpha \\left( -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b)) \\right)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "3wpBzZH_Xkvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate,epochs):\n",
        "        self.m = 100\n",
        "        self.b = -120\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X,y):\n",
        "        # calcualte the b using GD\n",
        "        for i in range(self.epochs):\n",
        "            loss_slope_b = -2 * np.sum(y - self.m*X.ravel() - self.b)\n",
        "            loss_slope_m = -2 * np.sum((y - self.m*X.ravel() - self.b)*X.ravel())\n",
        "\n",
        "            self.b = self.b - (self.lr * loss_slope_b)\n",
        "            self.m = self.m - (self.lr * loss_slope_m)\n",
        "        print(self.m,self.b)\n",
        "\n",
        "    def predict(self,X):\n",
        "        return self.m * X + self.b"
      ],
      "metadata": {
        "id": "eXr0HxNUPoBR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unQn1cPaPn95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gd = GDRegressor(0.001,50)\n",
        "\n",
        "gd.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbblGTAeUZmz",
        "outputId": "484cd861-3a11-46b2-9a07-d53584a52a85"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28.159367347119066 -2.3004574196824854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gd.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3xc1GlQVaGq",
        "outputId": "edb05c5c-c81a-4e0e-c55f-7ceb9e3b3cd7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6343842836315579"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uW_bkDCRZZvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Nb2au1BZZqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Types of Gradient Descent\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pt6pddOMaIK4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBGUBAQKZZmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W2o0VWaecARz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Batch Gradient Descent\n",
        "\n",
        "### Mean Squared Error Cost Function\n",
        "Batch Gradient Descent computes the gradient of the cost function using the entire dataset.\n",
        "\n",
        "$$\n",
        "J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( m \\)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial m} = -\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( b \\)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( m \\)\n",
        "$$\n",
        "m := m - \\alpha \\left( -\\frac{1}{n} \\sum_{i=1}^{n} x_i (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( b \\)\n",
        "$$\n",
        "b := b - \\alpha \\left( -\\frac{1}{n} \\sum_{i=1}^{n} (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Advantages\n",
        "- **Accuracy**: Provides a precise estimate of the gradient since it uses the entire dataset.\n",
        "- **Stable Updates**: Generally results in stable and smooth convergence to the minimum.\n",
        "\n",
        "### Disadvantages\n",
        "- **Computationally Expensive**: Can be very slow for large datasets as it requires processing the entire dataset for each update.\n",
        "- **Memory Usage**: Requires storing the entire dataset in memory, which may not be feasible for very large datasets."
      ],
      "metadata": {
        "id": "k0ypP2uvcA_B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LITO994ccYPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "jI1Whp7tcivl"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = load_diabetes(return_X_y=True) # returning x and y explicitly"
      ],
      "metadata": {
        "id": "aS9-A1dcciyb"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u0CvIr8ci1L",
        "outputId": "d174cb45-fbdb-4ed9-8db5-e5c3eb987db0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(442, 10)\n",
            "(442,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#X"
      ],
      "metadata": {
        "id": "DuswUXmNheme"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test ,y_train , y_test = train_test_split(X,y,test_size=0.2,random_state=2)"
      ],
      "metadata": {
        "id": "A253Ak_Ici4r"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg = LinearRegression()\n",
        "reg.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "gdCg4orjh5Ww",
        "outputId": "2a13d6f0-0adb-42d9-85f2-254f1228f717"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearRegression()"
            ],
            "text/html": [
              "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(reg.coef_)\n",
        "print(reg.intercept_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CMu_avrgSMD",
        "outputId": "c9972118-4952-4f4f-d0a9-cb109569a1a4"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[  -9.15865318 -205.45432163  516.69374454  340.61999905 -895.5520019\n",
            "  561.22067904  153.89310954  126.73139688  861.12700152   52.42112238]\n",
            "151.88331005254167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = reg.predict(X_test)\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVufMUsRgSTE",
        "outputId": "f124bd9e-bcd4-4de9-a507-7b4d2490cc8f"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4399338661568968"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5YuRNGKogSW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class for batch gradient descent"
      ],
      "metadata": {
        "id": "NKHBrN5LiFGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            # update all the coef and the intercept\n",
        "            y_hat = np.dot(X_train,self.coef_) + self.intercept_\n",
        "            #print(\"Shape of y_hat\",y_hat.shape)\n",
        "            intercept_der = -2 * np.mean(y_train - y_hat)\n",
        "            self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "            coef_der = -2 * np.dot((y_train - y_hat),X_train)/X_train.shape[0]\n",
        "            self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_"
      ],
      "metadata": {
        "id": "rWghRkDwgSch"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdr = GDRegressor(epochs =1000,learning_rate=0.5)\n",
        "\n",
        "gdr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZyTd96TgSfV",
        "outputId": "7841490a-8f8f-41ed-8ed1-d2be0ae70b53"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152.01351687661833 [  14.38990585 -173.7235727   491.54898524  323.91524824  -39.32648042\n",
            " -116.01061213 -194.04077415  103.38135565  451.63448787   97.57218278]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = gdr.predict(X_test)"
      ],
      "metadata": {
        "id": "esC67s1Xjfs0"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MkXAD2CjfpP",
        "outputId": "214110a0-21a2-4ecc-fe94-20e5e6e6f5aa"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4534503034722803"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IKc-_Fnujfm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YYmwyLZRjfjc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Stochastic Gradient Descent (SGD)\n",
        "\n",
        "Stochastic Gradient Descent updates the parameters using only one data point at a time.\n",
        "\n",
        "### Cost Function\n",
        "The cost function is the same as in Batch Gradient Descent:\n",
        "\n",
        "$$\n",
        "J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( m \\) (for a single data point)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial m} \\approx -x_i (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( b \\) (for a single data point)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} \\approx -(y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( m \\)\n",
        "$$\n",
        "m := m - \\alpha \\left( -x_i (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( b \\)\n",
        "$$\n",
        "b := b - \\alpha \\left( -(y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Advantages\n",
        "- **Efficiency**: Requires less memory and computational resources as it processes one data point at a time.\n",
        "- **Faster Convergence**: Can converge faster than Batch Gradient Descent due to frequent updates, which can be beneficial for large datasets.\n",
        "\n",
        "### Disadvantages\n",
        "- **Noisy Updates**: Updates can be noisy and may lead to oscillations or divergence if not properly tuned.\n",
        "- **Less Accurate**: The gradient is computed based on a single data point, which can lead to less accurate updates compared to Batch Gradient Descent."
      ],
      "metadata": {
        "id": "PZnpRMCVcjeB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AJsY7YL6cuvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## class for stochastic-gradient-descent"
      ],
      "metadata": {
        "id": "h6INmpIIkf17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SGDRegressor:\n",
        "\n",
        "    def __init__(self,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "            for j in range(X_train.shape[0]):\n",
        "                idx = np.random.randint(0,X_train.shape[0])\n",
        "\n",
        "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
        "\n",
        "                intercept_der = -2 * (y_train[idx] - y_hat)\n",
        "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
        "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_\n"
      ],
      "metadata": {
        "id": "TV1PDJ16curl"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd = SGDRegressor(learning_rate=0.01,epochs=40)"
      ],
      "metadata": {
        "id": "yhMz_c24cunR"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sgd.fit(X_train,y_train)\n",
        "y_pred = sgd.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YS0BtUgncukC",
        "outputId": "9bdaeec2-38e0-4746-ba45-aeb82ee344c2"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "160.19074995429037 [  62.16642768  -38.65675442  321.32806586  228.21971075   30.4041768\n",
            "   -5.46780503 -158.86115166  128.32518424  280.43720127  121.36138455]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JdPr3Eha72bw",
        "outputId": "8609f705-4900-490e-d6bc-b38f37872254"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4078132440185448"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BRI4Pukv72Wd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Mini-Batch Gradient Descent\n",
        "\n",
        "Mini-Batch Gradient Descent is a compromise between Batch and Stochastic Gradient Descent. It uses a subset of the data, called a mini-batch, for each update.\n",
        "\n",
        "### Cost Function\n",
        "The cost function is still the same:\n",
        "\n",
        "$$\n",
        "J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n} (y_i - (mx_i + b))^2\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( m \\) (for a mini-batch)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial m} = -\\frac{1}{m} \\sum_{i=1}^{m} x_i (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Gradient with Respect to \\( b \\) (for a mini-batch)\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial b} = -\\frac{1}{m} \\sum_{i=1}^{m} (y_i - (mx_i + b))\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( m \\)\n",
        "$$\n",
        "m := m - \\alpha \\left( -\\frac{1}{m} \\sum_{i=1}^{m} x_i (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Update Rule for \\( b \\)\n",
        "$$\n",
        "b := b - \\alpha \\left( -\\frac{1}{m} \\sum_{i=1}^{m} (y_i - (mx_i + b)) \\right)\n",
        "$$\n",
        "\n",
        "### Advantages\n",
        "- **Balanced Approach**: Balances the computational efficiency and convergence stability. It benefits from faster convergence while still providing more stable updates than SGD.\n",
        "- **Memory Efficient**: More memory-efficient than Batch Gradient Descent, especially for large datasets.\n",
        "\n",
        "### Disadvantages\n",
        "- **Tuning Required**: Requires careful tuning of the mini-batch size and learning rate to achieve optimal performance.\n",
        "- **Still Noisy**: While less noisy than SGD, the updates can still be somewhat noisy compared to Batch Gradient Descent.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-bmUahgHcvRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ELjkrjsdcx0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "class MBGDRegressor:\n",
        "\n",
        "    def __init__(self,batch_size,learning_rate=0.01,epochs=100):\n",
        "\n",
        "        self.coef_ = None\n",
        "        self.intercept_ = None\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def fit(self,X_train,y_train):\n",
        "        # init your coefs\n",
        "        self.intercept_ = 0\n",
        "        self.coef_ = np.ones(X_train.shape[1])\n",
        "\n",
        "        for i in range(self.epochs):\n",
        "\n",
        "            for j in range(int(X_train.shape[0]/self.batch_size)):\n",
        "\n",
        "                idx = random.sample(range(X_train.shape[0]),self.batch_size)\n",
        "\n",
        "                y_hat = np.dot(X_train[idx],self.coef_) + self.intercept_\n",
        "                #print(\"Shape of y_hat\",y_hat.shape)\n",
        "                intercept_der = -2 * np.mean(y_train[idx] - y_hat)\n",
        "                self.intercept_ = self.intercept_ - (self.lr * intercept_der)\n",
        "\n",
        "                coef_der = -2 * np.dot((y_train[idx] - y_hat),X_train[idx])\n",
        "                self.coef_ = self.coef_ - (self.lr * coef_der)\n",
        "\n",
        "        print(self.intercept_,self.coef_)\n",
        "\n",
        "    def predict(self,X_test):\n",
        "        return np.dot(X_test,self.coef_) + self.intercept_"
      ],
      "metadata": {
        "id": "sqreP8RScyAi"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mbr = MBGDRegressor(batch_size=int(X_train.shape[0]/50),learning_rate=0.01,epochs=100)"
      ],
      "metadata": {
        "id": "IUepX2c1cx6Q"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mbr.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6Yf4HO480PJ",
        "outputId": "6a0b52e2-3e2c-4add-c516-54a04c11a49c"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "152.61435435834505 [  29.12953505 -146.71650344  458.97700281  300.91376833  -28.46766282\n",
            "  -95.68329806 -192.80965602  110.09448874  406.26912159  124.58353199]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = mbr.predict(X_test)"
      ],
      "metadata": {
        "id": "8AEWZQ3a80NJ"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "r2_score(y_test,y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tfw6iQ480KR",
        "outputId": "d8b59f29-1afc-49b2-eb44-c517702c133d"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4550694086875028"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5fxF9Jf380G4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "- **Batch Gradient Descent**: Uses the entire dataset, providing precise gradients and stable convergence, but is computationally expensive and requires significant memory.\n",
        "- **Stochastic Gradient Descent (SGD)**: Uses single data points for updates, making it more memory-efficient and faster for large datasets, but can be noisy and less accurate.\n",
        "- **Mini-Batch Gradient Descent**: Offers a balance between accuracy and efficiency by using a subset of the data, combining benefits from both Batch and SGD but requiring careful tuning.\n",
        "\n",
        "Feel free to modify the learning rate \\(\\alpha\\) and other hyperparameters as needed for your specific problem."
      ],
      "metadata": {
        "id": "p5LoByNncyjo"
      }
    }
  ]
}